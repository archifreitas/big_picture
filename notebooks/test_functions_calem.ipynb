{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34496dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from big_picture import get_merged_data\n",
    "import pandas as pd\n",
    "raw_data = get_merged_data.get_data(\"../raw_data/data 12k/\").sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee7de72c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged columns for hp\n",
      "minor_preprocessing done\n",
      "lowercase\n",
      "nrs count\n",
      "remove_digits\n",
      "emotions\n",
      "punctuation\n",
      "de_emojify\n",
      "tokenize\n",
      "stopwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/calem/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lematizing\n",
      "vocab_richness\n"
     ]
    }
   ],
   "source": [
    "from big_picture.pre_processor import pre_process\n",
    "data = pre_process(raw_data,dataset='hp', all_true=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded5759d",
   "metadata": {},
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515889d4",
   "metadata": {},
   "source": [
    "from big_picture.label import Label\n",
    "label = Label(data,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785381f1",
   "metadata": {},
   "source": [
    "label.clusters[0].df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f164645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from big_picture.classifier import Classifier\n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750da2e8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(946, 768)\n",
      "(946, 14)\n",
      "Epoch 1/20\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 1.9548 - accuracy: 0.3779 - val_loss: 1.5993 - val_accuracy: 0.5359\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.4919 - accuracy: 0.5583 - val_loss: 1.5437 - val_accuracy: 0.5274\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 1.3491 - accuracy: 0.6003 - val_loss: 1.4105 - val_accuracy: 0.5654\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.1985 - accuracy: 0.5952 - val_loss: 1.4431 - val_accuracy: 0.6034\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.1773 - accuracy: 0.6123 - val_loss: 1.3758 - val_accuracy: 0.5907\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.9727 - accuracy: 0.6708 - val_loss: 1.3263 - val_accuracy: 0.6203\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.9741 - accuracy: 0.6511 - val_loss: 1.4439 - val_accuracy: 0.5570\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.9590 - accuracy: 0.6614 - val_loss: 1.3647 - val_accuracy: 0.5907\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.8936 - accuracy: 0.7191 - val_loss: 1.3678 - val_accuracy: 0.5992\n",
      "Epoch 10/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.8502 - accuracy: 0.7096 - val_loss: 1.2861 - val_accuracy: 0.6118\n",
      "Epoch 11/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.7306 - accuracy: 0.7324 - val_loss: 1.2884 - val_accuracy: 0.6160\n",
      "Epoch 12/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.7618 - val_loss: 1.4904 - val_accuracy: 0.5865\n",
      "Epoch 13/20\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.7212 - accuracy: 0.7501 - val_loss: 1.2647 - val_accuracy: 0.6203\n",
      "Epoch 14/20\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.5778 - accuracy: 0.7911 - val_loss: 1.5859 - val_accuracy: 0.5105\n",
      "Epoch 15/20\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.7192 - accuracy: 0.7341 - val_loss: 1.4166 - val_accuracy: 0.6160\n",
      "Epoch 16/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5030 - accuracy: 0.8073 - val_loss: 1.4617 - val_accuracy: 0.6160\n",
      "Epoch 17/20\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.5227 - accuracy: 0.8110 - val_loss: 1.4012 - val_accuracy: 0.6076\n",
      "Epoch 18/20\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.5259 - accuracy: 0.7977 - val_loss: 1.4343 - val_accuracy: 0.6245\n",
      "Epoch 19/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4003 - accuracy: 0.8531 - val_loss: 1.4058 - val_accuracy: 0.6329\n",
      "Epoch 20/20\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3753 - accuracy: 0.8868 - val_loss: 1.5794 - val_accuracy: 0.6118\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46b6799b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFBertForSequenceClassification: ['distilbert', 'dropout_19', 'pre_classifier']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['bert']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bd47bdff0a5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/JoaoCalem/big_picture/big_picture/classifier.py\u001b[0m in \u001b[0;36mdivide_labels\u001b[0;34m(self, world)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'level_0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msa_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/JoaoCalem/big_picture/big_picture/label.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, label, vec_name, model_name, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'kmeans'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             self.clusters= self.kmeans(df, \n\u001b[0m\u001b[1;32m     52\u001b[0m                                   \u001b[0;34m'news_all_data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                   \u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/JoaoCalem/big_picture/big_picture/label.py\u001b[0m in \u001b[0;36mkmeans\u001b[0;34m(self, X, column, vectors, clusters, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/sklearn/cluster/_k_means.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0;31m# avoid forcing order when copy_x=False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m         X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n\u001b[0m\u001b[1;32m    858\u001b[0m                         order=order, copy=self.copy_x)\n\u001b[1;32m    859\u001b[0m         \u001b[0;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    537\u001b[0m                     \u001b[0;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "classifier.divide_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66804947",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.labels[0].clusters[0].df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f6dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from big_picture.vectorizers import embedding_strings\n",
    "X = embedding_strings(data.news_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0b6707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {0: 'Activism',\n",
    "               1: 'Business',\n",
    "               2: 'Crime',\n",
    "               3: 'Culture',\n",
    "               4: 'Education',\n",
    "               5: 'Entertainment',\n",
    "               6: 'Health',\n",
    "               7: 'Media',\n",
    "               8: 'Other',\n",
    "               9: 'Politics',\n",
    "               10: 'Religion',\n",
    "               11: 'Science',\n",
    "               12: 'Sports',\n",
    "               13: 'Technology',\n",
    "               14: 'Trends',\n",
    "               15: 'World News'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6978c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = {key: [] for key in labels_dict.keys()}\n",
    "for i, result in enumerate(classifier.model.predict(X)):\n",
    "    for j, label_pred in enumerate(result):\n",
    "        if label_pred >= classifier.threshold:\n",
    "            labels[j].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6643f7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "6\n",
      "23\n",
      "0\n",
      "4\n",
      "281\n",
      "23\n",
      "17\n",
      "295\n",
      "3\n",
      "0\n",
      "32\n",
      "3\n",
      "77\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for label in labels.values():\n",
    "    print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7e9488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
